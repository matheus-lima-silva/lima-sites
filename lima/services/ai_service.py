"""
M√≥dulo para integra√ß√£o com servi√ßos de IA (OpenAI/Gemini).
Este m√≥dulo cont√©m fun√ß√µes para envio de prompts e recebimento de respostas
dos servi√ßos de IA para formatar mensagens humanizadas.
"""
import logging
import json
from typing import Dict, Any, Optional, List

import httpx

from ..settings import Settings

settings = Settings()
logger = logging.getLogger(__name__)

# Configura√ß√µes padr√£o para os modelos de IA
DEFAULT_MODEL = "gpt-3.5-turbo"
DEFAULT_TEMPERATURE = 0.7
DEFAULT_MAX_TOKENS = 300
DEFAULT_TIMEOUT = 30.0  # segundos


class AIServiceError(Exception):
    """Exce√ß√£o para erros relacionados ao servi√ßo de IA"""
    pass


async def format_endereco_resposta(endereco: Dict[str, Any]) -> str:
    """
    Utiliza IA para formatar uma resposta humanizada para um endere√ßo encontrado.
    
    Args:
        endereco: Dicion√°rio com dados do endere√ßo
        
    Returns:
        str: Texto formatado de forma humanizada
    """
    # Constr√≥i um prompt baseado no endere√ßo
    prompt = f"""
    Formate o seguinte endere√ßo de forma humanizada e amig√°vel, 
    como se estivesse ajudando algu√©m a encontrar este local:
    
    Logradouro: {endereco.get('logradouro', 'N√£o informado')}
    N√∫mero: {endereco.get('numero', 'N√£o informado')}
    Bairro: {endereco.get('bairro', 'N√£o informado')}
    Cidade: {endereco.get('municipio', 'N√£o informado')}
    UF: {endereco.get('uf', 'N√£o informado')}
    CEP: {endereco.get('cep', 'N√£o informado')}
    
    Inclua informa√ß√µes sobre como esse endere√ßo est√° registrado em nosso sistema.
    Seja conciso mas amig√°vel, como se estivesse conversando por WhatsApp.
    """
    
    # Se existir coordenadas, adicione ao prompt
    if endereco.get('latitude') and endereco.get('longitude'):
        prompt += f"\nObserva√ß√£o: Este endere√ßo tem coordenadas geogr√°ficas registradas: {endereco.get('latitude')}, {endereco.get('longitude')}."
    
    # Use o servi√ßo de IA para formatar a resposta
    try:
        response_text = await query_openai(prompt)
        return response_text
    except AIServiceError as e:
        logger.warning(f"Falha ao usar IA para formatar resposta: {e}")
        
        # Fallback para resposta formatada manualmente
        return (
            f"üè† *Endere√ßo encontrado*\n\n"
            f"*Logradouro:* {endereco.get('logradouro', 'N√£o informado')}, "
            f"{endereco.get('numero', 'S/N')}\n"
            f"*Bairro:* {endereco.get('bairro', 'N√£o informado')}\n"
            f"*Cidade:* {endereco.get('municipio', 'N√£o informado')}\n"
            f"*UF:* {endereco.get('uf', 'N√£o informado')}\n"
            f"*CEP:* {endereco.get('cep', 'N√£o informado')}\n\n"
            f"Para sugerir altera√ß√µes neste endere√ßo, use o comando 'sugerir'."
        )


async def format_sugestao_resposta(sugestao: Dict[str, Any]) -> str:
    """
    Utiliza IA para formatar uma resposta humanizada para confirmar o recebimento de uma sugest√£o.
    
    Args:
        sugestao: Dicion√°rio com dados da sugest√£o
        
    Returns:
        str: Texto formatado de forma humanizada
    """
    # Constr√≥i um prompt baseado na sugest√£o
    prompt = f"""
    Formate uma mensagem amig√°vel confirmando o recebimento da sugest√£o abaixo.
    Agrade√ßa ao usu√°rio pela contribui√ß√£o e explique que a sugest√£o ser√° analisada.
    
    Tipo de sugest√£o: {sugestao.get('tipo_sugestao', 'N√£o informado')}
    Detalhes: {sugestao.get('detalhe', 'N√£o informado')}
    
    Seja conciso mas amig√°vel, como se estivesse conversando por WhatsApp.
    """
    
    # Use o servi√ßo de IA para formatar a resposta
    try:
        response_text = await query_openai(prompt)
        return response_text
    except AIServiceError as e:
        logger.warning(f"Falha ao usar IA para formatar resposta: {e}")
        
        # Fallback para resposta formatada manualmente
        return (
            f"‚úÖ *Sugest√£o registrada com sucesso!*\n\n"
            f"Sua sugest√£o ser√° analisada por nossa equipe e voc√™ receber√° uma notifica√ß√£o "
            f"quando for processada.\n\n"
            f"*Conte√∫do da sugest√£o:*\n{sugestao.get('detalhe', 'N√£o informado')}\n\n"
            f"Agradecemos sua contribui√ß√£o para mantermos nossa base de dados atualizada!"
        )


async def query_openai(
    prompt: str, 
    model: str = DEFAULT_MODEL,
    temperature: float = DEFAULT_TEMPERATURE,
    max_tokens: int = DEFAULT_MAX_TOKENS
) -> str:
    """
    Envia uma consulta para a API da OpenAI e retorna a resposta.
    
    Args:
        prompt: O texto do prompt a ser enviado
        model: O modelo da OpenAI a ser usado
        temperature: Controle de aleatoriedade (0.0-2.0)
        max_tokens: N√∫mero m√°ximo de tokens na resposta
        
    Returns:
        str: Texto da resposta da IA
        
    Raises:
        AIServiceError: Se ocorrer um erro na chamada da API
    """
    # Verifica se a chave da API est√° configurada
    api_key = getattr(settings, "OPENAI_API_KEY", None)
    if not api_key:
        raise AIServiceError("Chave da API da OpenAI n√£o configurada")
        
    # Prepara os dados da requisi√ß√£o
    url = "https://api.openai.com/v1/chat/completions"
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    
    payload = {
        "model": model,
        "messages": [
            {"role": "system", "content": "Voc√™ √© um assistente √∫til que formata respostas de forma humanizada e amig√°vel para um sistema de busca de endere√ßos via WhatsApp."},
            {"role": "user", "content": prompt}
        ],
        "temperature": temperature,
        "max_tokens": max_tokens
    }
    
    try:
        async with httpx.AsyncClient() as client:
            response = await client.post(
                url, 
                headers=headers, 
                json=payload,
                timeout=DEFAULT_TIMEOUT
            )
            response.raise_for_status()
            result = response.json()
            
            # Extrai o texto da resposta
            if "choices" in result and len(result["choices"]) > 0:
                return result["choices"][0]["message"]["content"].strip()
            else:
                raise AIServiceError("Formato de resposta inv√°lido da OpenAI")
    except httpx.HTTPStatusError as e:
        logger.error(f"Erro de API da OpenAI: {e.response.text}")
        raise AIServiceError(f"Erro da API da OpenAI: {e.response.status_code}")
    except httpx.RequestError as e:
        logger.error(f"Erro de requisi√ß√£o para OpenAI: {str(e)}")
        raise AIServiceError(f"Erro de conex√£o com a API da OpenAI: {str(e)}")
    except Exception as e:
        logger.error(f"Erro inesperado ao chamar a API da OpenAI: {str(e)}")
        raise AIServiceError(f"Erro ao processar resposta da OpenAI: {str(e)}")


async def query_gemini(
    prompt: str,
    temperature: float = DEFAULT_TEMPERATURE,
    max_tokens: int = DEFAULT_MAX_TOKENS
) -> str:
    """
    Envia uma consulta para a API do Gemini (Google) e retorna a resposta.
    Implementa√ß√£o alternativa que pode ser usada em vez da OpenAI.
    
    Args:
        prompt: O texto do prompt a ser enviado
        temperature: Controle de aleatoriedade (0.0-1.0)
        max_tokens: N√∫mero m√°ximo de tokens na resposta
        
    Returns:
        str: Texto da resposta da IA
        
    Raises:
        AIServiceError: Se ocorrer um erro na chamada da API
    """
    # Verifica se a chave da API est√° configurada
    api_key = getattr(settings, "GEMINI_API_KEY", None)
    if not api_key:
        raise AIServiceError("Chave da API do Gemini n√£o configurada")
        
    # API do Gemini
    url = f"https://generativelanguage.googleapis.com/v1/models/gemini-pro:generateContent?key={api_key}"
    headers = {
        "Content-Type": "application/json"
    }
    
    payload = {
        "contents": [
            {
                "parts": [
                    {
                        "text": prompt
                    }
                ]
            }
        ],
        "generationConfig": {
            "temperature": temperature,
            "maxOutputTokens": max_tokens,
            "topP": 0.95,
            "topK": 40
        }
    }
    
    try:
        async with httpx.AsyncClient() as client:
            response = await client.post(
                url, 
                headers=headers, 
                json=payload,
                timeout=DEFAULT_TIMEOUT
            )
            response.raise_for_status()
            result = response.json()
            
            # Extrai o texto da resposta (formato espec√≠fico do Gemini)
            if "candidates" in result and len(result["candidates"]) > 0:
                candidate = result["candidates"][0]
                if "content" in candidate and "parts" in candidate["content"]:
                    parts = candidate["content"]["parts"]
                    if parts and "text" in parts[0]:
                        return parts[0]["text"].strip()
                        
            # Se chegou aqui, n√£o conseguiu extrair a resposta
            raise AIServiceError("Formato de resposta inv√°lido do Gemini")
    except httpx.HTTPStatusError as e:
        logger.error(f"Erro de API do Gemini: {e.response.text}")
        raise AIServiceError(f"Erro da API do Gemini: {e.response.status_code}")
    except httpx.RequestError as e:
        logger.error(f"Erro de requisi√ß√£o para Gemini: {str(e)}")
        raise AIServiceError(f"Erro de conex√£o com a API do Gemini: {str(e)}")
    except Exception as e:
        logger.error(f"Erro inesperado ao chamar a API do Gemini: {str(e)}")
        raise AIServiceError(f"Erro ao processar resposta do Gemini: {str(e)}")